{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YIhTxlefmr0R"
   },
   "source": [
    "# Advances in Data Mining\n",
    "## Assignment 2\n",
    "\n",
    "**Course**: \tAdvances in Data Mining  \n",
    "**Year**: \t2019-2020  \n",
    "**Assignment**:\t2 (Local sensitive hashing)  \n",
    "**Authors**: \t[Alexandre Cirilo](mailto://s2377640@umail.leidenuniv.nl) (2377640)  \n",
    "**Program**: \tComputer Science (Bioinformatics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this assignment, we are provided with a file (*user_movie.npy*) consisting of about 65 million rows and 2 columns, each row reflecting the relation: ```<user_id, movie_id>``` (i.e. user *x* watched movie *y*). In total, there are 103703 unique users, and 17700 unique movies. This data comprehends an extract from the Netflix challenge.\n",
    "\n",
    "\n",
    "The aim of this work is to find, with the help of LSH (**Local Sensitive Hashing**), pairs of users whose jaccard similarity is higher than 0.5.\n",
    "\n",
    "Here, we resort to LSH instead of the brute-force search to find the aforementioned pairs of users since the latter would force us to calculate the Jaccard similarity score for about $5.377 \\times 10^{9}$ pairs (i.e. about 5 billion pairs), which would be too expensive.\n",
    "\n",
    "#### Data Preprocessing\n",
    "\n",
    "First and foremost, we need to preprocess the data in such a way that we can represent it in a **sparse** matrix format (in this assignment, we chose to use the CSR - *Compressed Sparse Row* - format) that will be our *characteristic matrix*. In this matrix, columns represent users and rows represent movies.\n",
    "\n",
    "If a specific user has seen a determinate movie, then: ```sparse(user, movie) = 1```; meaning that our (Boolean) matrix will hold value **1** (*True*) whenever a user has seen a movie, and **0** otherwise (*False*).\n",
    "\n",
    "We chose to use a sparse matrix to represent our data since the data itself is quite sparse, i.e. all users have watched between 300 and 3000 movies (out of the total 17700 movies), meaning that most entries in the matrix will be zero. By using a sparse matrix, we are only storing non-zero values, assuming the remaining entries are zeros. This approach saves a lot of memory and computing time. In fact, you can often encounter such matrices when working with NLP (Neuro-Linguistic Programming) or machine learning tasks. More specifically, we chose to use the CSR format for the sparse matrix because of its efficient item access and slicing, and fast arithmetics.\n",
    "\n",
    "#### Minhashing\n",
    "\n",
    "With minhashing, one aims to build a signature matrix which is composed of *p* rows, respective to the number of permutations we perform on the rows of the original characteristic (sparse) matrix (i.e. movies), and *c* columns, where each column refers to one single user (thus, we will have 103703 columns, in this assignment).\n",
    "\n",
    "To perform minhashing, as said beforehand, we perform *p* permutations on the rows of the characteristic (sparse) matrix. Then, for a determinate permutation, and for each column (i.e. for each user), the minhash value will be equal to the index of the first row containing a non-zero value (i.e. 1; that is, a movie an user has seen). We repeat this process for every user, *p* times. In the end, we will have generated a signature matrix of size $p \\times c$, where $p$ refers to the number of permutations, and $c$ to the number of users in the dataset.\n",
    "\n",
    "#### LSH\n",
    "\n",
    "Our goal here is to find the most similar pairs of users above a certain threshold $t$; similarity between users is defined by the number of movies they have seen in common. The train of thought is that we are only going to focus on the pairs that are most likely to be similar, avoiding the need of an expensive brute-force search by not evaluating the similarity between pairs that are not likely to share attributes (in this case, movies seen by both users in a determinate pair). The general approach to such problem is to turn to *local sensitive hashing* (LSH), also termed *near-neighbour search*.\n",
    "\n",
    "The main idea to LSH is to apply a hash function, in this case, to each user (taking into account the movies they have seen), such that similar users (meaning they share several movies they both watched) are more likely to be hashed to the same bucket. Like so, we are trying to minimize hashing dissimilar users to the same bucket, in such a fashion that we only need to compute the Jaccard similarity score for each user pair combination - *candidate pairs* - in a given bucket. Hopefully, only the most similar users will end up in the same bucket, and if one or more dissimilar user(s) also end up in that particular bucket, we call them *false positives*. This way, we save up a lot of time, since, in the ideal case, we are only calculating the similarity between users likely to be similar.\n",
    "\n",
    "##### Banding Technique\n",
    "\n",
    "Following the generation of the signature matrix, one can divide it into **_b_** bands consisting of **_r_** rows each. For each band, each column will be hashed to a specific bucket. Here, columns with the same tuple (vector of integers of length *r* corresponding to the minhash values for each user in a specific band) will be hashed to the same bucket. Note that we use a separate bucket array for each band to avoid hashing columns with the same vector in different band to the same bucket. As expected, this approach makes similar columns much more likely to be candidate pairs than dissimilar ones.\n",
    "\n",
    "###### Probabilities\n",
    "\n",
    "1. Probability that the signatures agree in all rows of one particular band: $s^{r}$.\n",
    "2. Probability that the signatures disagree in at least one row of a particular bnad: $1-s^{r}$.\n",
    "3. Probability that the signatures disagree in at least one row of each of the bands: $(1-s^{r})^{b}$.\n",
    "4. Probability that the signatures agree in all the rows of at least one band, and thus become a candidate pair: $1-(1-s^{r})^{b}$.\n",
    "\n",
    "Regardless of the chosen constants *b* and *r*, the function has the form of an *S-curve* in which the threshold *t* (i.e. similarity score at wich the probability of becoming a candidate pair is 1/2) is a function of *b* and *r*. A good approximation to the threshold is:\n",
    "\n",
    "\\begin{equation}\n",
    "s=(1/b)^{\\frac{1}{r}}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, since the threshold in this work is fixed to 0.5, we want to approximate the equation shown above to 0.5. Some settings that approximate to 0.5 are listed in the table below.\n",
    "\n",
    "| *s* | *b*|*r*| *p* |\n",
    "|:---:|:--:|:-:|:---:|\n",
    "| 0.5 | 16 | 4 |  64 |\n",
    "| 0.5 | 20 | 5 | 100 |\n",
    "| 0.5 | 24 | 6 | 216 |\n",
    "\n",
    "After having tried different combinations of *b* and *r*, we chose to stick with: $b=20$ and $r=5$. Using shorter values for *r* takes up much more time at the jaccard similarity score computation step since it will generate more candidate pairs (lower *r* implies higher probability of hashing users to the same bucket, and thus many more pairs need to be evaluated). On the other hand, using higher values for *b*, notwithstanding higher number of pairs to be evaluated, also takes up more time since we need to generate more permutations (e.g. 216 instead of 100, in the case shown in the table above).\n",
    "\n",
    "#### Jaccard Similarity\n",
    "\n",
    "The Jaccard similarity (coefficient) of a set is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cap B|}{|A| + |B| - A \\cap B|}\n",
    "\\end{equation}\n",
    "\n",
    "For the calculation of the Jaccard similarity score, in the current implementation, we will take advantage of the *set* data structure in Python (which is described as an \"unordered collections of unique elements\" in the documentation) which allow for fast intersection and union calculation between two sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "In this assignment, the workflow enumerated below will be followed in order to find pairs of similar users (users that have seen several movies in common) above a threshold of 0.5:\n",
    "\n",
    "0. (<font color='red'>Checkpoint</font>: check that at least on Jaccard similarity calculation method is picked)\n",
    "\n",
    "1. **Load** the data from *user_movie.npy*.\n",
    "    1. <font color='red'>Checkpoint</font>: check if $p = b \\times r$ and $t \\in [0, 1]$.\n",
    "2. **Preprocess** the data:\n",
    "    1. Create the *seen_per_user* list which holds, for each user, the list of movies seen by that user. This variable will be assessed to check for the set of movies watched by a given user instead of checking the sparse matrix later on in an attempt to speed up the whole process.\n",
    "    2. Generate a sparse matrix in *CSR* format (**SciPy**) with columns representing users, rows representing movies, with values of 1 (if user *x* has seen movie *y*), and 0 otherwise.\n",
    "    3. <font color='red'>Checkpoint</font>: check if the shape of the sparse matrix is in agreement with the number of users (columns) and movies (rows).\n",
    "3. **Minhash** the data:\n",
    "    1. Initialize the signature matrix by calling *np.zeros(...)* (**NumPy**) with shape ($p$, $nu$), where $p$ is the number of permutations and $nu$ the number of users.\n",
    "    2. <font color='blue'>Multiprocessing</font>: parallellize (pool with 8 processes in our system) the generation of the signature matrix. This is repeated $p$ times (i.e. number of permutations) to generate the signature matrix. Using multiprocessing here turns out to be around 1.7x faster than the canonical approach, as shown in the plot below (signature length *vs.* runtime for 6 different signature lengths).\n",
    "        1. Generate the permutations of rows in the characteristic (sparse) matrix.\n",
    "        2. Return the minhash value for each user.\n",
    "    3. Populate the empty signature matrix created in step 1 with each generated row of the signature matrix, accordingly.\n",
    "    4. <font color='red'>Checkpoint</font>: check if the shape of the signature matrix is in agreement with the number of permutations (rows) and users (columns).\n",
    "4. **LSH**:\n",
    "    1. Split the signature matrix in $b$ matrices, where $b$ corresponds to the number of bands.\n",
    "    2. Initialize a list of $b$ (number of bands) bucket dictionaries where each bucket dictionary holds several entries with:\n",
    "        - Key: Hashed tuple value (i.e. column respective to a specific user in a specific band).\n",
    "        - Value: User with hashed tuple of value *key*.\n",
    "    3. Initialize an empty dictionary *tmp* where:\n",
    "        - Key: Hashed value of a specific bucket.\n",
    "        - Value: 1.\n",
    "    4. For each bucket in each band, hash the entire bucket (i.e. set of users contained in that bucket). The hashed value will be the key for the *tmp* dictionary, such that ```tmp[key] = 1```.\n",
    "    5. Repeat the process for every bucket and at each step, check ```tmp[key]``` already exists. If it does, it means we have two identical buckets in different bands (i.e. buckets with exactly the same content; list of users). It is pointless to re-evaluate potential candidate pairs present in identical buckets; we only want to check them once. Thus, if we find another bucket identical to a previous one, we remove it from the from the bucket array (by removing it from the dictionary that represent the band in which it appeared).\n",
    "    6. Generate *candidate pairs* for each band using combinations of users without repetition in each bucket. Note that only buckets containing 2 or more users will be considered in this step, since we need 2 or more users to generate candidate pairs.\n",
    "    7. Candidate pairs are added to the ```candidates_vector``` variable (type *set* to hold unique pairs only).\n",
    "5. **Compute Jaccard similarity**:\n",
    "    The ```jaccard``` function defined in the ```LSH``` class accepts the ```candidates_vector``` variable defined in the previous step, the signature matrix, as well as 3 arguments that define how the Jaccard similarity score is calculated:\n",
    "    - ```on_sig (bool)```: if *True*, similarity is calculated directly on the signature matrix taking into account two sets taken from the columns that represents each user (one column in the signature matrix represents one single user), respectively.\n",
    "    - ```on_sparse (bool)```: if *True*, similarity is calculated on the sparse matrix using the ```seen_per_user``` variable (list containing the movies seen per user, i.e. *user ID* equals the index in the list).\n",
    "    - ```on_both (bool)```: if *True*, similarity is first calculated on the signature matrix and then proofread by calculating the similarity on the resulting pairs.\n",
    "    1. Initialize variables:\n",
    "        1. ```self.pairs_dict```: dictionary that will keep reference to all the pairs found for each of the selected Jaccard similarity calculation methods.\n",
    "        2. ```JS_calc```: list that holds the boolean values respective to each Jaccard similarity calculation method (*True* if method will be performd, *False* otherwise).\n",
    "    2. For each chosen Jaccard similarity calculation method:\n",
    "        1. Create a ```self.iterable``` variable consisting of:\n",
    "            - User IDs (for *user1* and *user2* in the pair).\n",
    "            - Sets of movies seen for each user.\n",
    "            - Threshold (in this case: 0.5).\n",
    "        2. Call the ```jaccard_parallel``` function: also has a default keyword argument ```on_both_step``` (*True* if first step of the 'on_both' Jaccard similarity calculation method, in which case we do not want to add the pairs found to the 'pairs_dict' variable yet, i.e. we still need to proofread those pairs on the sparse matrix first).\n",
    "            1. Creates a pool of processes run in parallel (pool with 8 processes in our system).\n",
    "            2. Calls the ```similarity``` function from ```parallelize.py``` which calculates the Jaccard similarity score (on candidate pairs found in the ```self.iterable``` variable) and returns a tuple if a given pair of users has a similarity score equal or above the set threshold, otherwise returns *None*.\n",
    "            3. Cleaning up the output:\n",
    "                1. Cleaning up pairs (removes pairs with score lower than threshold).\n",
    "                2. Sort by first value in tuple (i.e. *user1*).\n",
    "                3. Append found sets of pairs to the ```self.pairs_dict``` variable \n",
    "                   according to the chosen calculation method.\n",
    "       3. Call the ```save``` function:\n",
    "           - ```file``` argument: name of the file to be generated.\n",
    "           - ```data```: pairs of similar users found (uses the variable ```self.pairs_dict[method]```, where *method* is on of the following: *on_sig*, *on_sparse*, or *on_both*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- Pivotal to this assignment, is the notion that the probability that the minhash function for a random permutation of rows produces the same value for two sets approximates the Jaccard similarity of those particular sets.\n",
    "- With regards to the hash functions and the generated buckets (for each band), the standard Python hash function (```hash(...)```)  will be used, and the buckets will be represented by the hashed values themselves. As stated in the documentation: \"they are used to quickly compare dictionary keys during a dictionary lookup\", making it appropriate for our problem.\n",
    "- To tackle the long runtimes observed during the Jaccard similarity calculation step in our implementation, we decided to take advantage of the **multiprocessing** module. Even though it was not encouraged as stated in the feedback for the assignment 1 of the Advances in Data Mining course (Fall 2019), we observe a substantial decrease in runtime when using a pool of 8 processes that parallelize the calculation of the similarity score on a set of candidate pairs (this was performed with the 'on_sparse' approach only). Hence, we decided to use it in 2 occasions:\n",
    "    1. During the generation of the signature matrix, to generate the index of permuted rows of the characteristic (sparse) matrix, and return the first occurence of a non-zero value in the subsequent matrix for each user (```permute``` function).\n",
    "    2. During the calculation of the Jaccard similarity score between pairs of users (```similarity``` function).\n",
    "- Both functions listed above are stored in the **_parallelize.py_** file. Thus, we need to import ```parallelize``` first to be able to call them.\n",
    "- Our implementation outputs 3 files (in the case where we decide to set 'on_sig' ,'on_sparse', and 'on_both' to *True*). Note that only **ans.txt** (output from 'on_both') should be taken into account. The remaining two files are there solely for discussion and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ratio w/:w/o multiprocessing: ~1.71x faster.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "# Timings (ran on different signature lengths for 'on_sparse')\n",
    "parallel, serial = [121.04, 129.90, 145.12, 162.95, 174.79, 186.67], [183.88, 225.09, 268.36, 283.025, 308.81, 313.56]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rect1 = plt.bar([i*10-1 for i in range(5, 11)], parallel, width=2, alpha=0.75, label=\"With multiprocessing\")\n",
    "rect2 = plt.bar([i*10+1 for i in range(5, 11)], serial, width=2, alpha=0.75, label=\"Without multiprocessing\")\n",
    "plt.legend()\n",
    "ax.set_xlabel(\"Signature length\")\n",
    "ax.set_ylabel(\"Runtime (s)\")\n",
    "\n",
    "# Average ratios w/:w/o multiprocessing for each signature length\n",
    "avg = [s/p for p, s in zip(parallel, serial)]\n",
    "print(\"Average ratio w/:w/o multiprocessing: ~{}x faster.\".format(round(np.mean(avg), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "The code itself consists in a class ```LSH``` containing multiple functions that act as building blocks and follow the workflow stated beforehand. At every occasion deemed necessary or non-trivial, comments are added directly above, or next to, the line of code in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9ENge0xciQ3"
   },
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    \n",
    "    def __init__(self, p, b, r, t, on_sig, on_sparse, on_both):\n",
    "        \"\"\"\n",
    "        p: Number of permutations (i.e. signature length)\n",
    "        b: Number of bands\n",
    "        r: Number of rows\n",
    "        t: Threshold (takes a value between 0 and 1)\n",
    "        \"\"\"\n",
    "        self.p, self.b, self.r, self.t = p, b, r, t\n",
    "        self.on_sig, self.on_sparse, self.on_both = on_sig, on_sparse, on_both\n",
    "        \n",
    "        # Check if given parameters are acceptable\n",
    "        try:\n",
    "            assert p ==  b * r and 0 <= t <= 1, \"Failed\"\n",
    "        except AssertionError as error:\n",
    "            print(error)\n",
    "            return\n",
    "\n",
    "        # Loading 'user_movie.npy'\n",
    "        self.load(file=\"user_movie.npy\")\n",
    "    \n",
    "    def load(self, file):\n",
    "        \"\"\"\n",
    "        Loads the user_movie.npy array and assigns it to the variable 'data'.\n",
    "        \"\"\"\n",
    "        data = np.load(file)\n",
    "\n",
    "        # Preprocessing data\n",
    "        print(\"Preprocessing data...\", end=\"\\t\\t\\t\")\n",
    "        return self.preprocess(data)\n",
    "    \n",
    "    def preprocess(self, data):\n",
    "        \"\"\"\n",
    "        Creates a list of movies seen per user (self.seen_per_user) and generates a sparse matrix.\n",
    "        \"\"\"\n",
    "        # Number of users and movies\n",
    "        self.nu, self.nm = len(np.unique(data[:, 0])), len(np.unique(data[:, 1]))\n",
    "        # User IDs (and range), Movie IDs\n",
    "        (self.users, self.indices), self.movies = np.unique(data[:, 0], return_counts=True), np.unique(data[:, 1])\n",
    "        \n",
    "        # 'seen_per_user' list keeps a list of the movies seen by each user\n",
    "        # e.g. seen_per_user[0] = [...] (for user ID = 0)\n",
    "        start, self.seen_per_user = 0, []\n",
    "        for index in np.cumsum(self.indices, dtype=int):\n",
    "            self.seen_per_user.append(data[start:index, 1])\n",
    "            start = index\n",
    "        print(\"OK\")\n",
    "        \n",
    "        print(\"Generating sparse matrix...\", end=\"\\t\\t\")\n",
    "        # Generating data to populate sparse matrix in CSR format\n",
    "        rows = list(chain(*self.seen_per_user))                                             # Movies (1D)\n",
    "        cols = [[i] * len(self.seen_per_user[i]) for i in range(len(self.seen_per_user))]   # Users\n",
    "        cols = list(chain(*cols))                                                           # Users (1D)\n",
    "        data = [True] * len(rows)                                                           # Set 'True' if seen\n",
    "        # Populating sparse matrix (Compressed Sparse Row format)\n",
    "        sparse = csr_matrix((data, (rows, cols)), dtype=bool)\n",
    "        print(\"OK\")\n",
    "        \n",
    "        try:\n",
    "            print(\"\\vCheck sparse matrix shape...\", end=\"\\t\\t\")\n",
    "            assert sparse.shape  == (self.nm, self.nu), \"Failed\"\n",
    "            print(\"OK\")\n",
    "        except AssertionError as error:\n",
    "            print(error)\n",
    "            return\n",
    "        \n",
    "        # MinHashing\n",
    "        return self.minhash(sparse)\n",
    "    \n",
    "    def minhash(self, sparse):\n",
    "        \"\"\"       \n",
    "        The Jaccard similarity coefficient is a commonly used indicator of the similarity between two sets.\n",
    "        Two sets are more similar (i.e. have relatively more members in common) when their Jaccard index is closer to 1.\n",
    "        The goal of MinHashing is to estimate J(A,B) quickly, without explicitly computing the intersection and union.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Generating signature matrix...\", end=\"\\t\\t\")\n",
    "        # Initialize signature matrix\n",
    "        sig = np.zeros((self.p, self.nu))\n",
    "\n",
    "        # Permute 'p' times the rows (i.e. movies) of the sparse matrix\n",
    "        # And get the first non-zero (i.e. 'True') element for each column (i.e. user)\n",
    "        with mp.Pool(mp.cpu_count()) as pool:\n",
    "            iterable = [(permutation, sparse, sig) for permutation in range(self.p)]\n",
    "            tmp = pool.starmap(parallelize.permute, iterable)\n",
    "        # Each permutation (one minhash value for each user) is a row in the resulting signature matrix\n",
    "        for permutation, row in tmp:\n",
    "            sig[permutation] = row\n",
    "        # Delete unused variables\n",
    "        del iterable, tmp\n",
    "        print(\"OK\")\n",
    "        \n",
    "        try:\n",
    "            print(\"\\vCheck signature matrix shape...\", end=\"\\t\")\n",
    "            assert sparse.shape  == (self.nm, self.nu), \"Failed\"\n",
    "            print(\"OK\")\n",
    "        except AssertionError as error:\n",
    "            print(error)\n",
    "            return\n",
    "        \n",
    "        return self.lsh(sig, sparse)\n",
    "\n",
    "    def lsh(self, sig, sparse):\n",
    "        \"\"\"\n",
    "        Divides matrix M into b bands of r rows.\n",
    "        For each band, hash each column to a hash table with k buckets.\n",
    "        Candidate column pairs are those that hash to the same bucket for â‰¥ 1 band.\n",
    "        Values of b and r can be tuned to catch most similar pairs and few nonsimilar pairs.\n",
    "        \n",
    "        LSH involves a tradeoff:\n",
    "        If avoidance of false negatives is important, select b and r to produce a threshold lower than t.\n",
    "        If speed is important and you wish to limit false positives, select b and r to produce a higher threshold.\n",
    "        \"\"\"\n",
    "        # Split signature matrix according to defined b bands\n",
    "        matrices = np.array_split(sig, self.b)\n",
    "        \n",
    "        # 'buckets' is a list of dictionaries with keys as hash values containing users with respective hash\n",
    "        buckets = [{} for i in range(self.b)]\n",
    "        # Generating and filling buckets        \n",
    "        print(\"Hashing users...\", end=\"\\t\\t\\t\")\n",
    "        for band in range(self.b):\n",
    "            buckets[band] = collections.defaultdict(set)\n",
    "            # Each band is a list of hashed tuples (each tuple is a column, i.e. a user)\n",
    "            for user in self.users:\n",
    "                key = hash(tuple(matrices[band][:, user]))\n",
    "                # Users with the same hash value go to the same bucket\n",
    "                buckets[band][key].add(user)\n",
    "        print(\"OK\\t{}\".format(sum([len(buckets[i].keys()) for i in range(len(buckets))])))\n",
    "        \n",
    "        print(\"Hashing buckets...\", end=\"\\t\\t\\t\")\n",
    "        tmp = {}\n",
    "        for band in range(self.b):\n",
    "            for key in list(buckets[band].keys()):\n",
    "                # If hash value of bucket already exists, delete this bucket\n",
    "                # (to avoid checking identical bucket)\n",
    "                try:\n",
    "                    tmp[hash(frozenset(buckets[band][key]))]\n",
    "                    del buckets[band][key]\n",
    "                except:\n",
    "                    tmp[hash(frozenset(buckets[band][key]))] = True\n",
    "        # Keep reference of the number of buckets\n",
    "        self.n_buckets = len(tmp)\n",
    "        # Delete unused variable\n",
    "        del tmp\n",
    "        \n",
    "        print(\"OK\\t{}\".format(self.n_buckets))\n",
    "        \n",
    "        # Check potential candidates in buckets\n",
    "        candidates_vector = set()\n",
    "        print(\"Generating candidate pairs...\", end=\"\\t\\t\")\n",
    "        for band in range(self.b):\n",
    "            for key in list(buckets[band].keys()):\n",
    "                # Only check buckets with two or more candidates (pairs)\n",
    "                if len(list(buckets[band].keys())) >= 2:\n",
    "                    candidates = list(combinations(buckets[band][key], 2))\n",
    "                    for (user1, user2) in candidates:\n",
    "                        candidates_vector.add((user1, user2))\n",
    "        print(\"OK\\t{}\".format(len(candidates_vector)))\n",
    "        \n",
    "        # Compute Jaccard similarity on candidates\n",
    "        self.jaccard(candidates_vector, sig, self.on_sig, on_sparse, self.on_both)\n",
    "    \n",
    "    def jaccard(self, candidates_vector, sig, on_sig, on_sparse, on_both):\n",
    "        \"\"\"\n",
    "        Computes the Jaccard similarity score on the candidate pairs and dumps it\n",
    "        in CSV format in a .txt file.\n",
    "        Arguments:\n",
    "            - on_sig:    'True' if similarity is calculated on the signature matrix\n",
    "            - on_sparse: 'True' if similarity is calculated on the sparse matrix\n",
    "            - on_both:   'True' if similarity is calculated on the sparse matrix \n",
    "                                (after checking similarity on signature matrix)\n",
    "        \"\"\"\n",
    "        print(\"Computing Jaccard similarity...\", end=\"\\t\\t\")\n",
    "        # 'self.pairs_dict' holds all the pairs found for each JS calculation method\n",
    "        self.pairs_dict = {\"on_sig\": [], \"on_sparse\": [], \"on_both\": []}\n",
    "        # JS_calc list keeps track of which JS calculation methods were selected\n",
    "        JS_calc = [on_sig, on_sparse, on_both]\n",
    "        \n",
    "        for method in self.pairs_dict.keys():\n",
    "            # Compute JS score only in selected methods (i.e. if 'True' or 1)\n",
    "            if locals()[method]:\n",
    "                # If JS calculated on signature matrix\n",
    "                if method is \"on_sig\":\n",
    "                    # Keep record of starting point (time) of JS calculation\n",
    "                    timer_start = time.time()\n",
    "                    # Generate one set for each user of length 'p' (i.e. number of permutations)\n",
    "                    sig_sets = [set(i) for i in sig.T]\n",
    "                    # Generate 'self.iterable' for parallelization of JS calculation\n",
    "                    self.iterable = [(user1, user2, sig_sets[user1], sig_sets[user2], self.t) \n",
    "                                for (user1, user2) in candidates_vector]\n",
    "                    # Run JS calculation in parallel\n",
    "                    self.jaccard_parallel(method)\n",
    "                    # Dump found pairs to .txt file in CSV format\n",
    "                    self.save(file=\"ans_sig.txt\", data=self.pairs_dict[\"on_sig\"])\n",
    "                    # Determine elapsed time for JS calculation and store it in 'times' variable\n",
    "                    times[\"on_sig\"] = time.time()-timer_start\n",
    "                \n",
    "                if method is \"on_sparse\":\n",
    "                    timer_start = time.time()\n",
    "                    \"\"\"\n",
    "                    If jaccard similarity score calculated on the characteristic (sparse) matrix\n",
    "                    Here, we take advantage of the variable 'seen_per_user' that keeps the movies seen by a specific user\n",
    "                    e.g. seen_per_user[0] = [29, (...)], where '0' refers to the user ID = 0\n",
    "                    \"\"\"\n",
    "                    self.iterable = [(user1, user2, self.seen_per_user[user1], self.seen_per_user[user2], self.t)\n",
    "                                for (user1, user2) in candidates_vector]\n",
    "                    self.jaccard_parallel(method)\n",
    "                    self.save(file=\"ans_spr.txt\", data=self.pairs_dict[\"on_sparse\"])\n",
    "                    times[\"on_sparse\"] = time.time()-timer_start\n",
    "                        \n",
    "                if method is \"on_both\":\n",
    "                    timer_start = time.time()\n",
    "                    sig_sets = [set(i) for i in sig.T]\n",
    "                    self.iterable = [(user1, user2, sig_sets[user1], sig_sets[user2], self.t) \n",
    "                                for (user1, user2) in candidates_vector]\n",
    "                    self.jaccard_parallel(method, on_both_step=True)\n",
    "                    # Calculate JS similarity score on sparse matrix after checking similarity in signature matrix\n",
    "                    self.iterable = [(user1, user2, self.seen_per_user[user1], self.seen_per_user[user2], self.t)\n",
    "                                for (user1, user2) in self.pairs]\n",
    "                    self.jaccard_parallel(method)\n",
    "                    self.save(file=\"ans.txt\", data=self.pairs_dict[\"on_both\"])\n",
    "                    times[\"on_both\"] = time.time()-timer_start\n",
    "        \n",
    "        # Finished computing JS score\n",
    "        print(\"OK\", end=\"\\t\")\n",
    "        \n",
    "        # Print number of pairs found for each chosen method\n",
    "        for method in self.pairs_dict.keys():\n",
    "            if locals()[method]:\n",
    "                print(\"{}\".format(len(self.pairs_dict[method])), end=\" \")\n",
    "        \n",
    "        # Log progress\n",
    "        logger[\"p:{} b:{} r:{} t:{}\".format(self.p, self.b, self.r, self.t)] = \\\n",
    "            self.n_buckets, len(candidates_vector), len(self.pairs_dict[\"on_sig\"]), \\\n",
    "            len(self.pairs_dict[\"on_sparse\"]), len(self.pairs_dict[\"on_both\"])\n",
    "    \n",
    "    def jaccard_parallel(self, method, on_both_step=False):\n",
    "        \"\"\"\n",
    "        Creates a pool of 'x' processes (where 'x' equals the number of cores of the system).\n",
    "        This reduces the time taken to compute the JS score between candidate pairs (self.iterable).\n",
    "        \"\"\"\n",
    "        with mp.Pool(mp.cpu_count()) as pool:\n",
    "            self.pairs = pool.starmap(parallelize.similarity, self.iterable)\n",
    "        \n",
    "        \"\"\"\n",
    "        Process output:\n",
    "            1. Cleaning up pairs (removes pairs with score lower than threshold).\n",
    "               (i.e. when parallelize.similarity returns 'None)\n",
    "            2. Sort by first value in tuple (i.e. user1).\n",
    "            3. Append found sets of pairs to the 'self.pairs_dict' variable \n",
    "               according to the chosen calculation method.\n",
    "        \"\"\"\n",
    "        self.pairs = [i for i in self.pairs if i]\n",
    "        self.pairs = sorted(self.pairs, key=lambda x: x[0])\n",
    "        self.pairs = set(self.pairs)\n",
    "        # 'on_both' intermediate processing step is not appended to the final pairs\n",
    "        # Needs to be processed further after evaluation of JS score on sparse matrix\n",
    "        if not on_both_step:\n",
    "            self.pairs_dict[method] = self.pairs\n",
    "\n",
    "    def save(self, file, data):\n",
    "        \"\"\"\n",
    "        Saves data in CSV format to a .txt file.\n",
    "        \"\"\"\n",
    "        with open(file, mode=\"w\") as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=\",\", lineterminator=\"\\n\")\n",
    "            for row in data:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "Qz11dBGh04vZ",
    "outputId": "e8cce933-8755-424e-c092-c41480bdca08",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  p</th><th style=\"text-align: right;\">  b</th><th style=\"text-align: right;\">  r</th><th style=\"text-align: right;\">  t</th><th>on_sig  </th><th>on_sparse  </th><th>on_both  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\"> 20</td><td style=\"text-align: right;\">  5</td><td style=\"text-align: right;\">0.5</td><td>False   </td><td>False      </td><td>True     </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\t\t\tOK\n",
      "Generating sparse matrix...\t\tOK\n",
      "\u000b",
      "Check sparse matrix shape...\t\tOK\n",
      "Generating signature matrix...\t\tOK\n",
      "\u000b",
      "Check signature matrix shape...\tOK\n",
      "Hashing users...\t\t\tOK\t1174865\n",
      "Hashing buckets...\t\t\tOK\t293822\n",
      "Generating candidate pairs...\t\tOK\t39025323\n",
      "Computing Jaccard similarity...\t\tOK\t526 "
     ]
    }
   ],
   "source": [
    "# Import 'parallelize.py' which includes the 'permute' and 'similarity' functions\n",
    "import csv, tabulate, collections, multiprocessing as mp, parallelize, time\n",
    "from scipy.sparse import csr_matrix\n",
    "from itertools import combinations, chain\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Setting random seed generator for reproducibility\n",
    "np.random.seed(seed=2019)\n",
    "\n",
    "\"\"\"\n",
    "logger:         Logs all the relevant data (buckets, pairs of users found)\n",
    "times:          Keeps track of the time it took to run a given setting\n",
    "setting:        Tune this variable to change the paramenters of the LSH algorithm (p, b, r, t, on_sparse, and on_sig)\n",
    "  - p:          Signature length\n",
    "  - b:          Number of bands\n",
    "  - r:          Number of rows per band\n",
    "  - t:          Threshold (between 0 to 1)\n",
    "  - on_sig:     True if Jaccard similarity (JS) score calculated on the signature matrix\n",
    "  - on_sparse:  True if Jaccard similarity (JS) score calculated on the sparse matrix\n",
    "  - on_both:    True if Jaccard similarity (JS) score calulated on the sparse matrix after filtering through the signature matrix\n",
    "\n",
    "    List of settings in the form [p, b, r, t, on_sig, on_sparse, on_both]\n",
    "    i.e. [[settings_1], [settings_2], etc.], where settings_x=[p, b, r, t, on_sig, on_sparse, on_both]\n",
    "\"\"\"\n",
    "\n",
    "global logger, times\n",
    "logger, times, settings = {}, {\"on_sig\": 0, \"on_sparse\": 0, \"on_both\": 0}, [[100, 20, 5, 0.5, False, False, True]]\n",
    "\n",
    "for p, b, r, t, on_sig, on_sparse, on_both in settings:\n",
    "    # At least one JS calculation method must be set to 'True'\n",
    "    assert on_sig or on_sparse or on_both, \"At least one JS calculation method must be set to 'True'\"\n",
    "    \n",
    "    # Display table with chosen parameters\n",
    "    params = [[p, b, r, t, on_sig, on_sparse, on_both]]\n",
    "    headers=[\"p\", \"b\", \"r\", \"t\", \"on_sig\", \"on_sparse\", \"on_both\"]\n",
    "    display(HTML(tabulate.tabulate(params, headers, tablefmt=\"html\")))\n",
    "    \n",
    "    # LSH implementation\n",
    "    LSH(p, b, r, t, on_sig, on_sparse, on_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-processing\n",
    "\n",
    "Here we simply sort the output files by the value of the first column (i.e. \"user1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [\"ans_sig.txt\", \"ans_spr.txt\", \"ans.txt\"]\n",
    "\n",
    "for ans in files:\n",
    "    # Only check files that exist\n",
    "    try:\n",
    "        tmp = pd.read_csv(ans, names=[\"user1\", \"user2\"])\n",
    "        tmp = tmp.sort_values(\"user1\")\n",
    "        tmp.to_csv(ans, header=None, index=False)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & Discussion\n",
    "\n",
    "#### Summary of output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the table shown below, the 'on_sig' method found the highest number of pairs, followed by the 'on_sparse' and 'on_both' methods which differ by a mere 15 pairs.\n",
    "At first glance, the 'on_sig' method results feel overwhelming, but we do need to check for false positives first before jumping to any conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Buckets</th>\n",
       "      <th>Candidate pairs</th>\n",
       "      <th>Pairs (sig)</th>\n",
       "      <th>Pairs (sparse)</th>\n",
       "      <th>Pairs (both)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p:100 b:20 r:5 t:0.5</th>\n",
       "      <td>293822</td>\n",
       "      <td>39025323</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Buckets  Candidate pairs  Pairs (sig)  Pairs (sparse)  \\\n",
       "p:100 b:20 r:5 t:0.5   293822         39025323            0               0   \n",
       "\n",
       "                      Pairs (both)  \n",
       "p:100 b:20 r:5 t:0.5           526  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers=[\"Buckets\", \"Candidate pairs\", \"Pairs (sig)\", \"Pairs (sparse)\", \"Pairs (both)\"]\n",
    "pd.DataFrame.from_dict(logger, orient=\"index\", columns=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regards to runtime (in seconds), clearly the 'on_sig' method is the fastest of them all (with the settings used in this execution of the implementation), followed by the 'on_both' method, and at last, by the 'on_sparse' method. The 'on_both' method was around 1.7x faster than the 'on_sparse' method, as emphasized by the barplot shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame.from_dict(times, orient=\"index\", columns=[\"Runtime (s)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JS score calculation on 'on_both' is ~{}x faster than on 'on_sparse'.\".format(round(times[\"on_sparse\"]/times[\"on_both\"], 2)))\n",
    "print(\"It took ~{} minutes for the algorithm to run using the 'on_both' approach.\".format(int(times[\"on_both\"]/60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plotting the runtimes\n",
    "rect1 = plt.bar(times.keys(), times.values(), alpha=0.75)\n",
    "plt.xlabel(\"Jaccard similarity score calculation method\")\n",
    "plt.ylabel(\"Runtime (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truly, the 'on_sig' approach to calculate the Jaccard similarity score was the fastest of them all. But when we take a look at the false positive percentage - *fp%* - we notice that, in reality, only about 0.01% (i.e. 100-99.99) of the pairs found by this method are \"true\" pairs, i.e. pairs that really do have a Jaccard similarity score of 0.5 or more. Put in other words, the overwhelming majority of the pairs found in 'on_sig' are not actually similar users.\n",
    "\n",
    "On the other side, both 'on_sparse' and 'on_both' have no false positives whatsoever. This was expected and makes total sense, since both approaches ultimately check for the similarity between users on the sparse matrix itself. The trick here is that the 'on_both' approach finds a number of pairs of similar users that comes very close to the number of pairs found by the 'on_sparse' matrix (284 vs. 298) but without the cost of checking <u>all</u> candidate pairs on the sparse matrix. That is, in the 'on_both' method, we essentially first filter through the candidate pairs by checking for their similarity on the signature matrix, and then use the subsequent candidate pairs to evaluate their similarity on the sparse matrix. What happens is that, after we evaluate the similarity between users on the signature matrix step, we will end up with a much lower number of candidate pairs to be checked on the sparse matrix. Whilst if we would directly calculate the similarity between pairs on the sparse matrix, we would have to check for a much larger number of candidate pairs.\n",
    "\n",
    "In conclusion, at the cost of losing some potential pairs of similar users, by using the 'on_both' approach, we manage to find a close number of pairs of similar users, as compared to the ideal case ('on_sparse'), in a much shorter time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for ans in files:\n",
    "    # If file exists\n",
    "    try:\n",
    "        stats[ans] = dict()\n",
    "        pred = pd.read_csv(ans, names=[\"user1\", \"user2\"], usecols=[0, 1])\n",
    "        true = pd.read_csv(\"all_similar_pairs.txt\", names=[\"user1\", \"user2\"], usecols=[0, 1])\n",
    "\n",
    "        # To check for duplicates and count \"true\" pairs (above threshold)\n",
    "        tmp = pd.concat([pred, true])\n",
    "\n",
    "        # Proofread pairs (using 'all_similar_pairs.txt')\n",
    "        true_pairs = tmp.duplicated().sum()\n",
    "        true_pairs_percentage = round(true_pairs/len(true)*100, 2)\n",
    "\n",
    "        # False positives\n",
    "        false_pos = len(pred) - tmp.duplicated().sum()\n",
    "        false_pos_percentage = round(false_pos/len(pred)*100, 2)\n",
    "\n",
    "        # False negatives\n",
    "        false_neg = len(true) - tmp.duplicated().sum()\n",
    "        false_neg_percentage = round(false_neg/len(true) * 100, 2)\n",
    "\n",
    "        # Add to statistics\n",
    "        stats[ans][\"true_pairs\"], stats[ans][\"true_pairs%\"] = true_pairs, true_pairs_percentage\n",
    "        stats[ans][\"fp\"], stats[ans][\"fp%\"] = false_pos, false_pos_percentage\n",
    "        stats[ans][\"fn\"], stats[ans][\"fn%\"] = false_neg, false_neg_percentage\n",
    "    \n",
    "    except FileNotFoundError as error:\n",
    "        print(error)\n",
    "\n",
    "# Show statistics\n",
    "pd.DataFrame.from_dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOQgkqQ8a4DE"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "With this work, we can confidently say that the using the **MinHash** algorithm provides us with a fast approximation to the Jaccard Similarity between two sets, by converting large sets - boolean arrays in the sparse matrix representing the movies seen by each user - to short signatures - MinHash signatures - while preserving similarity. Furthermore, we take advantage of **local sensitive hashing** which hashes similar users into the same buckets with high probability. This is much faster and efficient than an exhaustive search. Hence, in the context of massive datasets such as the one portrayed in this assignment, resorting to local sensitive hashing following the minhashing of our data is the most efficient solution to find a substantial number of similar pairs (in our case, pairs of users) in a relatively short amount of time (as per comparison with the brute-force approach).\n",
    "\n",
    "With regards to the S-curve described in the introduction section:\n",
    "- If avoiding false negatives is important (accuracy is important): \n",
    "    - Make $(1/b)^{\\frac{1}{r}}$ smaller than s (desired similarity)\n",
    "- If avoiding false positives is important (speed is important): \n",
    "    - Make $(1/b)^{\\frac{1}{r}}$ larger than s (desired similarity)\n",
    "\n",
    "On a final note, it took 12 minutes to find 284 pairs of similar users (using the settings [$p$:50 $b$:10 $r$:5 $t$:0.5] on the 'on_both' approach). Adding that to the preprocessing part of the implementation, which takes around 2-3 minutes, we end up  with around 14-15 minutes on average with the aforementioned parameters and 284 pairs. That is an acceptable number of similar pairs found in a considerably shorter time than in the 'on_sparse' approach (which only found 15 more pairs). To note, comparing the runtimes with the 'on_sig' approach is useless since it holds an overwhelming amount of false positives (derived from the similarity being evaluated on the signature matrix only, and not on the sparse matrix).\n",
    "\n",
    "As expected, using the following parameters: [$p$:100 $b$:20 $r$:5 $t$:0.5] did help in finding a higher number of similar pairs of users at the cost of a substantial increase in runtime, thus, [$p$:50 $b$:10 $r$:5 $t$:0.5] was picked instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Leskovec, Jure, et al. *Mining of Massive Datasets*. Cambridge University Press, 2015."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "AiDM_HA2_LSH.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
